{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from Run import RunBuilder as RB\n",
    "from Run import RunManager as RM\n",
    "from DataLoading import UdacityDataset as UD\n",
    "from DataLoading import ConsecutiveBatchSampler as CB\n",
    "\n",
    "from model import transferlearning_regnet_x as TL\n",
    "\n",
    "%run Visualization/Visualization.ipynb\n",
    "\n",
    "torch.set_printoptions(linewidth=120) \n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>1278.805481</td>\n",
       "      <td>1278.811508</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>1463.063530</td>\n",
       "      <td>2748.882675</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  epoch      loss  accuracy  epoch duration  run duration  \\\n",
       "0    1      1  0.007984  0.740741     1278.805481   1278.811508   \n",
       "1    1      2  0.033467  0.740741     1463.063530   2748.882675   \n",
       "\n",
       "   learning_rate  batch_size  num_workers  \n",
       "0          0.001          50            1  \n",
       "1          0.001          50            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a09394ec7648bfa74a996ee7b742ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = OrderedDict(\n",
    "    learning_rate = [0.001],\n",
    "    batch_size = [50],\n",
    "    num_workers = [1],\n",
    "    #shuffle = [True,False]\n",
    ")\n",
    "\n",
    "m = RM.RunManager()\n",
    "for run in RB.RunBuilder.get_runs(parameters):\n",
    "    network = TL.TLearning_regnetx()\n",
    "    network.cuda()\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(),lr = run.learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001/run.batch_size, amsgrad=False)\n",
    "# modify the time to visible time format, also use it to check the sequency of pictures is from former to current\n",
    "    udacity_dataset = UD.UdacityDataset(csv_file='/home/kxk190041/data/self_driving/train/interpolated.csv',\n",
    "                                     root_dir='/home/kxk190041/data/self_driving/train/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera')\n",
    "    dataset_size = int(len(udacity_dataset))\n",
    "    del udacity_dataset\n",
    "    split_point = int(dataset_size * 0.8)\n",
    "\n",
    "    training_set = UD.UdacityDataset(csv_file='/home/kxk190041/data/self_driving/train/interpolated.csv',\n",
    "                                     root_dir='/home/kxk190041/data/self_driving/train/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(0,split_point))\n",
    "\n",
    "    validation_set = UD.UdacityDataset(csv_file='/home/kxk190041/data/self_driving/train/interpolated.csv',\n",
    "                                     root_dir='/home/kxk190041/data/self_driving/train/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(split_point,dataset_size))\n",
    "    print(\"size of training set :{}\".format(len(training_set)))\n",
    "    print(\"size of validation set :{}\".format(len(validation_set)))\n",
    "    \n",
    "    training_cbs = CB.ConsecutiveBatchSampler(data_source=training_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    training_loader = DataLoader(training_set, sampler=training_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    validation_cbs = CB.ConsecutiveBatchSampler(data_source=validation_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    validation_loader = DataLoader(validation_set, sampler=validation_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    training_time = 0\n",
    "\n",
    "    m.begin_run( run,network,[run.batch_size,3,224,224] )\n",
    "    for epoch in range(10):\n",
    "        st_time = time.time()\n",
    "        m.begin_epoch()\n",
    "        \n",
    "        for training_sample in tqdm(training_loader):\n",
    "            training_sample['image'] = training_sample['image'].squeeze()\n",
    "            training_sample['image'] = torch.Tensor(resize(training_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "            param_values = [v for v in training_sample.values()]\n",
    "            image,angle = param_values[0],param_values[3]\n",
    "            image = image.to(device)\n",
    "            #print(image.dtype)\n",
    "            #image = image.to(torch.float)\n",
    "            prediction = network(image)\n",
    "            prediction = prediction.to(device)\n",
    "            labels = angle.to(device)\n",
    "            labels = labels.to(torch.float)\n",
    "            del param_values, image, angle\n",
    "            if labels.shape[0]!=prediction.shape[0]:\n",
    "                prediction = prediction[-labels.shape[0],:]\n",
    "            training_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "            optimizer.zero_grad()# zero the gradient that are being held in the Grad attribute of the weights\n",
    "            training_loss_angle.backward() # calculate the gradients\n",
    "            optimizer.step() # finishing calculation on gradient \n",
    "        print(\"Done\")\n",
    "        training_time += (time.time() - st_time) \n",
    "# Calculation on Validation Loss\n",
    "        with torch.no_grad():    \n",
    "            for Validation_sample in tqdm(validation_loader):\n",
    "                Validation_sample['image'] = Validation_sample['image'].squeeze()\n",
    "                Validation_sample['image'] = torch.Tensor(resize(Validation_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "\n",
    "                param_values = [v for v in Validation_sample.values()]\n",
    "                image,angle = param_values[0],param_values[3]\n",
    "                image = image.to(device)\n",
    "                prediction = network(image)\n",
    "                prediction = prediction.to(device)\n",
    "                labels = angle.to(device)\n",
    "                labels = labels.to(torch.float)\n",
    "                del param_values, image, angle\n",
    "                if labels.shape[0]!=prediction.shape[0]:\n",
    "                    prediction = prediction[-labels.shape[0],:]\n",
    "                validation_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "                m.track_loss(validation_loss_angle, labels.shape[0])\n",
    "                m.track_num_correct(prediction,labels) \n",
    "        m.end_epoch()\n",
    "        torch.save(network.state_dict(), \"/home/kxk190041/Self-Driving-Car/results/Angle_Adam_MSE_regnetx_Model-epoch-{}\".format(epoch))\n",
    "    m.end_run()\n",
    "m.save('result')\n",
    "print('Training time taken: ', training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Directly from disk\n",
    "\n",
    "tl_model = TL.TLearning_regnetx().to(device)\n",
    "tl_model.load_state_dict(torch.load('/home/kxk190041/Self-Driving-Car/results/Angle_Adam_MSE_regnetx_Model-epoch-9'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cnn(tl_model.pretrained.stem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udacity_dataset = UD.UdacityDataset(csv_file='/home/kxk190041/data/self_driving/train/interpolated.csv',\n",
    "                                 root_dir='/home/kxk190041/data/self_driving/train/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "\n",
    "# Load arbitrary data\n",
    "sample = udacity_dataset[3693]\n",
    "show_sample(sample)\n",
    "input_image = sample['image'].reshape(-1, 3, 480, 640).cuda()\n",
    "\n",
    "cam_extractor_tl = CamExtractorTLModel_regnetx(tl_model)\n",
    "\n",
    "# Forward pass\n",
    "model_output = tl_model(input_image)\n",
    "\n",
    "# Backward pass\n",
    "tl_model.zero_grad()\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(model_output, sample['angle'].cuda().reshape(1,1))\n",
    "loss.backward()\n",
    "\n",
    "cam_image = cam_extractor_tl.to_image(height=480, width=640) # Use this line to extract CAM image from the model!\n",
    "plt.imshow(cam_image[0, :, :], cmap='jet', alpha=0.5) # this shows CAM as overlay to the original input image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09da5c0f0cd944e8a1b92991cb83ad32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "169fc6dcfbf1453bb3f993e8f820fb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "  1%",
       "layout": "IPY_MODEL_cf39cd2fed3f4160b56b3648d03a035d",
       "max": 541,
       "style": "IPY_MODEL_cdb7f50858604b57ac7918c821d5fc76",
       "value": 4
      }
     },
     "231c4cfb7acf4255bf00331b651e4d23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8903b5f73cfb4e1d8c1b58a76a110ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_169fc6dcfbf1453bb3f993e8f820fb75",
        "IPY_MODEL_97870d47c3244e6b97dd316925455a01"
       ],
       "layout": "IPY_MODEL_a8e1b14a27fd48b587e88a8cd3d24a86"
      }
     },
     "97870d47c3244e6b97dd316925455a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_231c4cfb7acf4255bf00331b651e4d23",
       "style": "IPY_MODEL_09da5c0f0cd944e8a1b92991cb83ad32",
       "value": " 4/541 [00:09&lt;17:54,  2.00s/it]"
      }
     },
     "a8e1b14a27fd48b587e88a8cd3d24a86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cdb7f50858604b57ac7918c821d5fc76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "cf39cd2fed3f4160b56b3648d03a035d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
